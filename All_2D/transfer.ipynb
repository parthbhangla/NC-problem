{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ded93bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43ea670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class For Data Generation\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, path='./neuralzome_crate_local/2024-01-31-09-51-48/rgb/', batch_size=16, shuffle=True):\n",
    "        self.path = path\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        # Filter yaml with no annotations\n",
    "        self.images = []\n",
    "        for file in sorted(os.listdir(self.path)):\n",
    "            if file.endswith('.jpg'):\n",
    "                yaml_path = os.path.join(self.path, file.replace('.jpg', '.yaml'))\n",
    "                if os.path.exists(yaml_path):\n",
    "                    with open(yaml_path, 'r') as f:\n",
    "                        data = yaml.safe_load(f)\n",
    "                        if data.get('crates'):\n",
    "                            self.images.append(file)\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    # Length of the dataset\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.images) / self.batch_size))\n",
    "    \n",
    "    # Shuffling the dataset\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.images)\n",
    "\n",
    "    # Get a batch of data\n",
    "    def __getitem__(self, index):\n",
    "        batch_files = self.images[index * self.batch_size:(index + 1) * self.batch_size] # \n",
    "        images = []\n",
    "        keypoints = []\n",
    "\n",
    "        for file in batch_files:\n",
    "            img_path = os.path.join(self.path, file)\n",
    "            yaml_path = os.path.join(self.path, file.replace('.jpg', '.yaml'))\n",
    "\n",
    "            # Load image\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            img = np.array(img).astype('float32') / 255.0\n",
    "\n",
    "            # Load keypoints\n",
    "            with open(yaml_path, 'r') as f:\n",
    "                crate = yaml.safe_load(f)['crates'][0]\n",
    "            kp = np.array([\n",
    "                crate['x0'], crate['y0'],\n",
    "                crate['x1'], crate['y1'],\n",
    "                crate['x2'], crate['y2'],\n",
    "                crate['x3'], crate['y3']\n",
    "            ], dtype='float32')\n",
    "\n",
    "            images.append(img)\n",
    "            keypoints.append(kp)\n",
    "\n",
    "        return np.stack(images), np.stack(keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "893e1e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vs/h43bw_9d56z8mpjr0qs4v7nm0000gn/T/ipykernel_1152/918806619.py:11: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base_model = MobileNetV2(include_top=False, weights='imagenet', input_shape=(480, 640, 3))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 973ms/step - loss: 95029.7812 - mae: 270.9493\n",
      "Epoch 2/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 858ms/step - loss: 45657.0938 - mae: 172.4327\n",
      "Epoch 3/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - loss: 19099.4844 - mae: 111.2376\n",
      "Epoch 4/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - loss: 12585.7803 - mae: 86.7466\n",
      "Epoch 5/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 784ms/step - loss: 13747.6133 - mae: 93.6433\n",
      "Epoch 6/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 951ms/step - loss: 10881.3809 - mae: 79.3706\n",
      "Epoch 7/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - loss: 9950.5713 - mae: 79.2719\n",
      "Epoch 8/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 820ms/step - loss: 8445.9834 - mae: 73.6041\n",
      "Epoch 9/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 991ms/step - loss: 9772.1152 - mae: 77.3959\n",
      "Epoch 10/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - loss: 7874.7373 - mae: 69.5034 \n",
      "Epoch 11/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - loss: 8368.1104 - mae: 72.5382 \n",
      "Epoch 12/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 917ms/step - loss: 8241.7754 - mae: 71.5793\n",
      "Epoch 13/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 830ms/step - loss: 8247.2822 - mae: 71.9820\n",
      "Epoch 14/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - loss: 8404.9961 - mae: 70.9849 \n",
      "Epoch 15/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - loss: 8918.0762 - mae: 73.8744\n",
      "Epoch 16/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 940ms/step - loss: 8902.6113 - mae: 73.1460\n",
      "Epoch 17/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 964ms/step - loss: 7100.6982 - mae: 65.8521\n",
      "Epoch 18/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 681ms/step - loss: 7502.2734 - mae: 66.8633\n",
      "Epoch 19/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 862ms/step - loss: 8205.3701 - mae: 70.0176\n",
      "Epoch 20/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 825ms/step - loss: 7561.9209 - mae: 67.9986\n",
      "Epoch 21/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 854ms/step - loss: 7656.8506 - mae: 67.6878\n",
      "Epoch 22/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 902ms/step - loss: 7136.7461 - mae: 66.2015\n",
      "Epoch 23/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 887ms/step - loss: 7881.4712 - mae: 68.7379\n",
      "Epoch 24/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 780ms/step - loss: 7452.2549 - mae: 67.8814\n",
      "Epoch 25/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 711ms/step - loss: 6791.8447 - mae: 63.9374\n",
      "Epoch 26/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 812ms/step - loss: 6774.1782 - mae: 64.3974\n",
      "Epoch 27/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 966ms/step - loss: 7054.2568 - mae: 64.0167\n",
      "Epoch 28/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 954ms/step - loss: 6196.6152 - mae: 59.8011\n",
      "Epoch 29/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 925ms/step - loss: 6628.2139 - mae: 64.0355\n",
      "Epoch 30/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - loss: 6220.3608 - mae: 61.1722 \n",
      "Epoch 31/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 953ms/step - loss: 7000.1299 - mae: 66.4178\n",
      "Epoch 32/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 999ms/step - loss: 5823.0044 - mae: 60.1459\n",
      "Epoch 33/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - loss: 5482.8579 - mae: 57.8534\n",
      "Epoch 34/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 954ms/step - loss: 6185.3760 - mae: 60.5859\n",
      "Epoch 35/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - loss: 5598.6484 - mae: 59.2579\n",
      "Epoch 36/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 887ms/step - loss: 5546.2788 - mae: 57.1062\n",
      "Epoch 37/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - loss: 5941.9814 - mae: 59.7123\n",
      "Epoch 38/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - loss: 5367.2837 - mae: 56.2286\n",
      "Epoch 39/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - loss: 5672.1802 - mae: 59.7469\n",
      "Epoch 40/40\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 851ms/step - loss: 5107.1152 - mae: 56.0632\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x315c32ba0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import optimizers\n",
    "import tensorflow as tf\n",
    "\n",
    "# Custom loss\n",
    "def keypoint_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "# Load MobileNetV2 without top, with input shape (480, 640, 3)\n",
    "base_model = MobileNetV2(include_top=False, weights='imagenet', input_shape=(480, 640, 3))\n",
    "\n",
    "# Freeze layers if desired\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Build Sequential model and add base model as a layer\n",
    "model = models.Sequential()\n",
    "model.add(base_model)\n",
    "model.add(layers.GlobalAveragePooling2D())\n",
    "model.add(layers.Dense(1024, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(8))\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "    loss=keypoint_loss,\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "dataset = DataGenerator()\n",
    "\n",
    "# Train the model\n",
    "model.fit(dataset, epochs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7443c9db",
   "metadata": {},
   "source": [
    "# Left this here because resize works much better out of pocket"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
